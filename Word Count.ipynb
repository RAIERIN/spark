{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "\n",
    "Counting the number of occurances of words in a text is a popular first exercise using map-reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task\n",
    "\n",
    "Input: A text file consisting of words seperated by spaces.\n",
    "Output: A list of words and their counts, sorted from the most to the least common.\n",
    "\n",
    "We will use the book \"Moby Dick\" as our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the SparkContext\n",
    "from pyspark import SparkContext, SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a plan for pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_plan(rdd):\n",
    "    for x in rdd.toDebugString().decode().split('\\n'):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use textFile() to read the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 ms, sys: 506 Âµs, total: 1.63 ms\n",
      "Wall time: 22.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(\"file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for counting the words\n",
    "1. split line by spaces.\n",
    "2. map word to (word,1)\n",
    "3. count the number of occurances of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.69 ms, sys: 9.02 ms, total: 14.7 ms\n",
      "Wall time: 88.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words =     text_file.flatMap(lambda line: line.split(\" \"))\n",
    "not_empty = words.filter(lambda x: x!='') \n",
    "key_values= not_empty.map(lambda word: (word, 1)) \n",
    "counts=     key_values.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatMap()\n",
    "\n",
    "Note the line:\n",
    "\n",
    "words =     text_file.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "The reason we are using flatMap is that operation line.split(\" \") generates a list of strings, so had we used map the result would be an RDD of lists of words. Not an RDD of words. The difference between map and flatMap is that the second expects to get a list as the result from the map and  it concatenates the lists to form the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The execution plan\n",
    "\n",
    "In the last call we defined the execution plan, but we have not started to execute it.\n",
    "\n",
    "1. Preparing the plan took ~100ms, which is non-trivial amount of time,\n",
    "2. But much less than the time it will take to execute it.\n",
    "3. Lets have a look at the execution plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the details\n",
    "\n",
    "To see which step in the plan corresponds to which RDD we print out the execution plan for each of the RDDS.\n",
    "Note that the execution plan for words, not_empty and key_values are all the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[12] at RDD at PythonRDD.scala:53 []\n",
      " |  file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[13] at RDD at PythonRDD.scala:53 []\n",
      " |  file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(not_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[14] at RDD at PythonRDD.scala:53 []\n",
      " |  file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[15] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[11] at mapPartitions at PythonRDD.scala:133 []\n",
      " |  ShuffledRDD[10] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[9] at reduceByKey at <timed exec>:4 []\n",
      "    |  PythonRDD[8] at reduceByKey at <timed exec>:4 []\n",
      "    |  file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  file:///home/erin/Downloads/spark-3.0.1-bin-hadoop2.7/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Finally we count the number of times each word has occured. Now, finally, the Lazy execution model finally performs some actual work, which takes a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 6.73 ms, sys: 11.3 ms, total: 18 ms\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run #1\n",
    "Count=counts.count()  # Count = the number of different words\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y) # \n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amortization\n",
    "\n",
    "When the same commands are performed repeatedly on the same data, the execution time tends to decrease in later executions.\n",
    "\n",
    "The cells below are identical to the one above, with one exception at Run #3\n",
    "\n",
    "Observe that Run #2 take much less time than Run #1 Even though no cache() was explicitly requested. The reason is that Spark caches (or materializes) key_values, before executing reduceByKey() because performing reduceByKey requires a shuffle and shuffle requires that the input RDD materialzed. In other words, Sometime caching happens even if the programmer did not asked for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 11.7 ms, sys: 4.19 ms, total: 15.9 ms\n",
      "Wall time: 249 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##Run #2\n",
    "Count = counts.count()\n",
    "Sum = counts.map(lambda x:x[1]).reduce(lambda x,y: x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Run #3 we explicitly ask for counts to be cached. This will reduce the execution time in the following run Run #4 by a little bit, but not by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 7.25 ms, sys: 8.28 ms, total: 15.5 ms\n",
      "Wall time: 185 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run #3, cache\n",
    "Count=counts.cache().count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 5.73 ms, sys: 7.31 ms, total: 13 ms\n",
      "Wall time: 161 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run #4\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 8.45 ms, sys: 7.19 ms, total: 15.6 ms\n",
      "Wall time: 182 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run #5\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
